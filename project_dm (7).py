# -*- coding: utf-8 -*-
"""project_dm.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1x8b-bqPmpAwQdelTwxvFQ1txnWyIWKbM
"""

# Improting Libraries
import pandas as pd
import matplotlib.pyplot as plt
from wordcloud import WordCloud
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.metrics.pairwise import linear_kernel
import seaborn as sns
from surprise import Reader, Dataset, SVD

# Read the CSV file and create dataframe 
df_movies=pd.read_csv('/content/movies.csv')
df_ratings=pd.read_csv('/content/ratings.csv')

df_movies.info()

df_ratings.info()

df_movies.shape

df_ratings.shape

# Check for any null column
df_movies.isnull().sum()

df_ratings.isnull().sum()

df_movies.head()

df_ratings.head()

df_genres=df_movies['genres'].str.get_dummies(sep='|')
df_genres.head()

genres=[]
for genre in df_movies.genres:
    
    x=genre.split('|')
    for i in x:
         if i not in genres:
            genres.append(str(i))
genres=str(genres)    
movie_title=[]
for title in df_movies.title:
    movie_title.append(title[0:-7])
movie_title=str(movie_title)

wordcloud_genre=WordCloud(width=1500,height=800,background_color='black',min_font_size=2).generate(genres)
wordcloud_title=WordCloud(width=1500,height=800,background_color='cyan',min_font_size=2).generate(movie_title)

plt.figure(figsize=(15,10))
plt.axis('off')
plt.title('WORDCLOUD for Movies Genre',fontsize=30)
plt.imshow(wordcloud_genre)

plt.figure(figsize=(15,10))
plt.axis('off')
plt.title('WORDCLOUD for Movies title',fontsize=30)
plt.imshow(wordcloud_title)

# Merge the two dataframe
df_merge=pd.merge(df_ratings,df_movies, how='left',on='movieId')
df_merge.head()

plt.figure(figsize = (12, 8))
ax = sns.countplot(x="rating", data=df_merge)
for q in ax.patches:
    ax.annotate(str(q.get_height()), (q.get_x() * 1.01 , q.get_height() * 1.01))
ax.set_yticklabels([num for num in ax.get_yticks()])
plt.tick_params(labelsize = 15)
plt.title("Count Ratings in movie data", fontsize = 20)
plt.xlabel("Ratings", fontsize = 20)
plt.ylabel("Number of Ratings", fontsize = 20)
plt.grid()

df1=df_merge.groupby(['title'])[['rating']].sum()
high_rated_movies=df1.nlargest(10,'rating')
high_rated_movies.head()

ratings_mean_count = pd.DataFrame(df_merge.groupby('title')['rating'].mean())

ratings_mean_count['rating_counts'] = pd.DataFrame(df_merge.groupby('title')['rating'].count())

ratings_mean_count.head()

plt.figure(figsize=(10,8))
plt.rcParams['patch.force_edgecolor'] = True
sns.jointplot(x='rating', y='rating_counts', data=ratings_mean_count, alpha=0.4)

df2=df_merge.groupby('title')[['rating']].count()
rating_count_20=df2.nlargest(20,'rating')
rating_count_20.head()

plt.figure(figsize=(10,5))
plt.title('Top 20 movies with highest number of ratings',fontsize=20)
plt.xticks(fontsize=25,rotation=90)
plt.yticks(fontsize=25)
plt.xlabel('Movie Title',fontsize=30)
plt.ylabel('Ratings',fontsize=30)

plt.bar(rating_count_20.index,rating_count_20.rating,color='green')

# Tf-Idf vectorization 
tfidf_vector=TfidfVectorizer(stop_words='english')
tfidf_matrix=tfidf_vector.fit_transform(df_movies['genres'])

movie_user = df_merge.pivot_table(index='userId',columns='title',values='rating')
movie_user.head()

movie_user.fillna(0,inplace=True)
movie_user.head()

# Calculating cosine similarity
cosine_similarity = linear_kernel(tfidf_matrix, tfidf_matrix)

cosine_similarity.shape

indices=pd.Series(df_movies.index,index=df_movies['title'])
titles=df_movies['title']
def movie_recommendations(title):
    movie_index = indices[title]
    similarity_scores = list(enumerate(cosine_similarity[movie_index]))
    similarity_scores = sorted(similarity_scores, key=lambda x: x[1], reverse=True)
    similarity_scores = similarity_scores[1:21]
    movie_indices = [i[0] for i in similarity_scores]
    return titles.iloc[movie_indices]

movie_recommendations('Toy Story (1995)')

movie_recommendations('Champagne (1928)')

df_ratings.head()

reader = Reader()

data = Dataset.load_from_df(df_ratings[['userId', 'movieId', 'rating']], reader)

svd = SVD()

trainData = data.build_full_trainset()
svd.fit(trainData)

svd.predict(1, 101).est